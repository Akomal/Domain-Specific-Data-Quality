{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DED6311AkrNd"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index.legacy.embeddings.langchain import LangchainEmbedding\n",
        "\n",
        "import torch\n",
        "\n",
        "# Load documents\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "\n",
        "# Define system prompt\n",
        "system_prompt = \"\"\"\n",
        "You are a Q&A assistant. Your are going to answer questions as\n",
        "accurately as possible based on the instructions and context provided.\n",
        "\"\"\"\n",
        "\n",
        "# Define query wrapper prompt\n",
        "query_wrapper_prompt = PromptTemplate(\"{query_str}\")\n",
        "\n",
        "# Initialize LLM\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    device_map=\"auto\",\n",
        "    model_kwargs={\"torch_dtype\": torch.float16, \"load_in_8bit\": True}\n",
        ")\n",
        "\n",
        "# Initialize embeddings\n",
        "embeddings = LangchainEmbedding(\n",
        "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Create service context\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    chunk_size=1024,\n",
        "    llm=llm,\n",
        "    embed_model=embeddings\n",
        ")\n",
        "\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "query_engine = index.as_query_engine()\n",
        "input=input(\"Please enter your data quality issue: \")\n",
        "response=query_engine.query(input)\n",
        "print(response)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation\n",
        "# List of queries and their expected answers for evaluation\n",
        "queries_and_answers = [\n",
        "    {\n",
        "        \"query\": \"The currency column has these ($,€,&&, ߾) values. are these valid currencies\",\n",
        "        \"expected_answer\": \"Yes, $, €, and are valid currency symbols, but ߾ is not a standard currency symbol.\"\n",
        "    },\n",
        "\n",
        "     {\n",
        "        \"query\": \"How to check if the stocks data is not stale \",\n",
        "        \"expected_answer\": \"To determine if stock data is not too old and stale, you should use a combination of methods. First, check the timestamps to ensure data is recent. Implement version control to identify outdated data and use data freshness indicators like flags or counters. Regularly assess data quality for accuracy and completeness. Monitor user activity logs to see how frequently the data is accessed or updated. Evaluate performance metrics for anomalies in response times or error rates. Cross-reference your data with reliable external sources and set up automated monitoring systems with alerts for stale data.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Automate evaluation\n",
        "for item in queries_and_answers:\n",
        "    query = item[\"query\"]\n",
        "    expected_answer = item[\"expected_answer\"]\n",
        "    response = query_engine.query(query)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Response: {response}\")\n",
        "\n",
        "    # Evaluate response using Ragas\n",
        "    evaluation_result = evaluate(\n",
        "        metrics=[faithfulness, answer_relevance, answer_correctness],\n",
        "        llm=wrapped_llm,\n",
        "        embeddings=wrapped_embeddings,\n",
        "        query=query,\n",
        "        response=response,\n",
        "        ground_truth=expected_answer\n",
        "    )\n",
        "    print(f\"Evaluation Result: {evaluation_result}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jZWezrUNl_9e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}